一阶导数等于零表示函数斜率固定。
二阶导数没有特别的几何意义，通常可以根据二阶导数的符号变化，判断函数曲线的凹凸性及拐点，或用来判断所求驻点是否是极值点并且取得极大还是极小。二阶导数等于零说明此为函数的极点。



导数等于0说明函数在此处变化率为0，但不能说明在此处取得极值点。比如y=x³，y'=3x²，x=0时导数为0但x=0并不是极值点。

```
对于一元可微函数f (x)，它在某点x0有极值的充分必要条件是f(x)在x0的某邻域上一阶可导，在x0处二阶可导，且f'(X0)=0，f"(x0)≠0，那么：
1）若f"（x0）<0，则f在x0取得极大值；
2）若f"（x0）>0，则f在x0取得极小值。
```

```
一、直接法。先判断函数的单调性，若函数在定义域内为单调函数，则最大值为极大值，最小值为极小值

二、导数法
（1）求导数f'(x)；
（2）求方程f'(x)=0的根；
（3）检查f'(x)在方程的左右的值的符号，如果左正右负，那么f(x)在这个根处取得极大值；如果左负右正那么f(x)在这个根处取得极小值。

举例如下图：该函数在f'(x)大于0，f'(x)小于0，在f'(x)=0时，取极大值。同理f'(x)小于0，f'(x)大于0时，在f'(x)=0时取极小值。
```



二阶导数是一阶导数的导数，表示一阶导数的变化率。在几何上反映为该函数的凹凸性，二阶导数大于0为凹函数，小于0为凸函数。如一次函数，二阶导数等于0，既不是凹函数又不是凸函数。



# CLRM（classical linear regression model)古典线性回归模型假定：

1) **回归模型是参数线性的，但不一定是变量线性的**。 参数线性，例：Y=a+**b**X&sup2;、Y=a+**b**/X

变量线性，例：Y=a+b&sup2;X

2) **解释变量（X）与扰动误差项μ不相关**。

3) **扰动项的期望或均值为零**

即：E(μ|Xi)=0

4) **扰动项Ui的方差为常数或同方差**

即：var(Ui)= σ&sup2;

5) **无自相关，即两个误差项之间不相关**

Cov（μi，μj）=0

6） 观测次数必须要与待估计的参数个数

7） 解释变量要有变异性

8） 假定正确设定回归模型

9） 对于多变量复回归模型，**解释变量之间没有完全的线性关系**





线性回归是回归算法，而逻辑回归和softmax本质上是分类算法（从离散的分类目标导出），不过有一些场合下也有混着用的——如果目标输出值的取值范围和logistic的输出取值范围一致。

1、Linear Regression

可以说基本上是机器学习中最简单的模型了，但是实际上其地位很重要（计算简单、效果不错，在很多其他算法中也可以看到用LR作为一部分）。

先来看一个小例子，给一个“线性回归是什么”的概念。图来自[2]。

![这里写图片描述](https://img-blog.csdn.net/20150316214013445)![这里写图片描述](https://img-blog.csdn.net/20150316214000231) 

假设有一个房屋销售的数据如下： 

面积(m^2) 销售价钱（万元） 

```
123  250 
150  320 
87   160 
102  220
...  ...
```



当我们有很多组这样的数据，这些就是训练数据，我们希望学习一个模型，当新来一个面积数据时，可以自动预测出销售价格（也就是上右图中的绿线）；这样的模型必然有很多，其中最简单最朴素的方法就是线性回归，也就是我们希望学习到一个线性模型（上右图中的红线）。**不过说是线性回归，学出来的不一定是一条直线，只有在变量x是一维的时候才是直线，高维的时候是超平面。**

## 简单的线性回归(Linear Regression) - 最小二乘法 (least square method)

线性回归的作用: 当我们获得数据之后, 我们想要知道这些数据间元素的关系, 我们可以定义一个等式去描述这中关系. 这就是线性回归的作用. 

dependent variable: 就是要被预测的变量

Independent variable: 就是用来预测的变量



以下这个公式就是一个简单的线性回归的模型. 

beta 0 和 1 都是模型的变量

epsilon 是随机变量, 作为error term.  ( 个人理解: 因为现实生活中数据的预测结果可能被一些噪音所改变, 比如一个商店的销售额, 可能因为某天的某个客人很有钱而改变, 但是这种很有钱的客户很少见, 这种情况下的预测结果会有偏差, 使用epsilon 来进行校正. )

![img](https://img-blog.csdn.net/20160618013127742?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

可能的线性回归图例:

![img](https://img-blog.csdn.net/20160618013703999?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

线性回归的基本步骤:

![img](https://img-blog.csdn.net/20160618013949501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

我们通过 regression model 的到 regression equation, 然后使用 历史数据 对regression equation 的参数进行优化 得到 estimated regression equation. 获得最优参数, 进行预测新的independent 数据



例子:

背景. 一连锁饭店的 销售额 和 坐落在它周围的 大学的 学生数量 可能有关系 所以我们对 销售额 和 学生数量 之间的关系很感兴趣. 



我们收集了一部分历史数据. 如下

![img](https://img-blog.csdn.net/20160618040525234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

将数据转换为散点图:

![img](https://img-blog.csdn.net/20160618040757101?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



1.我们现在的问题 就能转化成 找到一条直线, 这条直线需要满足 使历史数据中的各个 x 所对应的 y 与 各个在直线上对应的y-head的差最小。

2.公式 :  ![img](https://img-blog.csdn.net/20160618041241155?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)yi 是 历史数据x对应的y ,y-head 是 x 对应的在直线上y的值.

3.根据estimation regression equation 我们知道

4.将 3 带入 2, 在对 b0 和 b1 分别求 偏导. 如图

![img](https://img-blog.csdn.net/20180405111951011)





# [linear regression and logistic regression](https://www.jianshu.com/p/48b5979c4d4a)

## ①linear regression

### target function的推导

线性回归是一种做拟合的算法：

![img](https://upload-images.jianshu.io/upload_images/10624272-9bdefd0970b6ce62.png)

通过工资和年龄预测额度，这样就可以做拟合来预测了。有两个特征，那么就要求有两个参数了，设置

![\theta_1,\theta_2](https://math.jianshu.com/math?formula=%5Ctheta_1%2C%5Ctheta_2)

，对应工资和年龄两个字段的值。拟合的公式一般都是

![s = wx + b](https://math.jianshu.com/math?formula=s%20%3D%20wx%20%2B%20b)

，所以还缺一个

![b](https://math.jianshu.com/math?formula=b)

，所以还要设置一个

![\theta_0](https://math.jianshu.com/math?formula=%5Ctheta_0)

，所以决策函数就是

![h_{\theta}(x) = \theta_0+\theta_1x_1+\theta_2x_2](https://math.jianshu.com/math?formula=h_%7B%5Ctheta%7D(x)%20%3D%20%5Ctheta_0%2B%5Ctheta_1x_1%2B%5Ctheta_2x_2)

。这个决策函数的形式并不是推出来的，而是经验之举设置成这个形式的。可以设置多一个

一个

![x_0](https://math.jianshu.com/math?formula=x_0)

，使得

![x_0=1](https://math.jianshu.com/math?formula=x_0%3D1)

，然后上式就可以变成

![h_{\theta}(x) = \theta_0x_0+\theta_1x_1+\theta_2x_2](https://math.jianshu.com/math?formula=h_%7B%5Ctheta%7D(x)%20%3D%20%5Ctheta_0x_0%2B%5Ctheta_1x_1%2B%5Ctheta_2x_2)

，整合一下，

![h_{\theta}(x) = \sum_{i=0}^{i = 2}\theta_ix_i=\theta^Tx](https://math.jianshu.com/math?formula=h_%7B%5Ctheta%7D(x)%20%3D%20%5Csum_%7Bi%3D0%7D%5E%7Bi%20%3D%202%7D%5Ctheta_ix_i%3D%5Ctheta%5ETx)

> > ### 误差
>
> 上面的![\theta_0](https://math.jianshu.com/math?formula=%5Ctheta_0)可以看做是一个误差，这个误差满足高斯分布，服从均值为0，方差为![\theta^2](https://math.jianshu.com/math?formula=%5Ctheta%5E2)的高斯分布，也就是高斯分布。**因为求出来的拟合数值不是一定就准确的，肯定会有一些的误差。**
>
> 
>
> ![img](https://upload-images.jianshu.io/upload_images/10624272-c1dffe4b0630427e.png)
>
> 一般误差不会太多，都是在0上下浮动的，所以0附近是最高的概率。
>
> 使用linear regression要满足三个条件：
>
> 1.独立，每一个样本点之间都要相互独立。
> 2.同分布，他们的银行是一样的，使用的算法是一样的。
> 3.误差都服从高斯分布。
>
> 

由于误差是服从高斯分布的，自然有：![P(\theta_0)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{\theta_0^2}{2\sigma^2})](https://math.jianshu.com/math?formula=P(%5Ctheta_0)%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp(-%5Cfrac%7B%5Ctheta_0%5E2%7D%7B2%5Csigma%5E2%7D))

上述式子有：![y = \theta^Tx+\theta_0](https://math.jianshu.com/math?formula=y%20%3D%20%5Ctheta%5ETx%2B%5Ctheta_0)

![P(\theta_0)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})](https://math.jianshu.com/math?formula=P(%5Ctheta_0)%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp(-%5Cfrac%7B(y_i-%5Ctheta%5ETx_i)%5E2%7D%7B2%5Csigma%5E2%7D))

需要求参数![\theta](https://math.jianshu.com/math?formula=%5Ctheta)，自然就是最大似然函数：


最大化这个似然函数那就是最小化![\frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-\theta^Tx_i)^2](https://math.jianshu.com/math?formula=%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5Em(y_i-%5Ctheta%5ETx_i)%5E2)。因为前面都是常数，可以忽略。而这个式子稍微变一下就是最小二乘法了。
**最小二乘法：![f = \frac{1}{2}\sum_{i=1}^m(h_{\theta}(x_i)-y_i)^2](https://math.jianshu.com/math?formula=f%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5Em(h_%7B%5Ctheta%7D(x_i)-y_i)%5E2)，不用上面化简出来的式子非要加上![\frac{1}{2}](https://math.jianshu.com/math?formula=%5Cfrac%7B1%7D%7B2%7D)是因为求导之后2是可以和0.5抵消的。**



。。。。。。未完待续

## ②[logistic regression](https://www.jianshu.com/p/48b5979c4d4a)

### target function 的推导

首先要提一个函数，**sigmoid函数：![f(x) = \frac{1}{1+e^{-z}}](https://math.jianshu.com/math?formula=f(x)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)**

这个函数之前被用来做神经网络的激活函数，但是它有一个缺点。

![img](https://upload-images.jianshu.io/upload_images/10624272-c12f53c13521c1da.png)

离原点处越远梯度就越趋近于0，在神经网络里面就会出现一个梯度消失的问题，所以后来就用relu或者tanh函数代替了。这个函数的值域是在

![(0, 1)](https://math.jianshu.com/math?formula=(0%2C%201))

，所以sigmoid函数可以把一个值映射成一个概率，通过这个函数就可以做出一个概率，最后用过这个概率来区分类别。这里的决策函数还是和之前的一样

![h(x) = \sum_{i=1}^m\theta_ix+\theta_0](https://math.jianshu.com/math?formula=h(x)%20%3D%20%5Csum_%7Bi%3D1%7D%5Em%5Ctheta_ix%2B%5Ctheta_0)

这样有点麻烦，可以把

![\theta_0](https://math.jianshu.com/math?formula=%5Ctheta_0)

合并到

![h(x)](https://math.jianshu.com/math?formula=h(x))

，只需要在

![x](https://math.jianshu.com/math?formula=x)

中加多一列1就好了，所以函数可以简化

![h(x) = \theta^Tx](https://math.jianshu.com/math?formula=h(x)%20%3D%20%5Ctheta%5ETx)

。这还是不是最终的决策函数，这只是一个值，只是一个得分，需要把这个得分转换成一个概率。所以最终的决策函数就是

![h(x) = \frac{1}{1+e^{-\theta^Tx}}](https://math.jianshu.com/math?formula=h(x)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D)

。假设有两个类别

![y=1,y=0](https://math.jianshu.com/math?formula=y%3D1%2Cy%3D0)

，那么可以假设属于

![y=1](https://math.jianshu.com/math?formula=y%3D1)

的类别就是

![p(y=1|x;\theta) = h(x)](https://math.jianshu.com/math?formula=p(y%3D1%7Cx%3B%5Ctheta)%20%3D%20h(x))

，属于

![y=0](https://math.jianshu.com/math?formula=y%3D0)

的类别就是

![p(y=0|x;\theta) = 1-h(x)](https://math.jianshu.com/math?formula=p(y%3D0%7Cx%3B%5Ctheta)%20%3D%201-h(x))

这样的两个的式子对于求导和化简都不好，所以合并一下：



因为y只有1和0，如果当前的分类是y，那么就是

![h(x)](https://math.jianshu.com/math?formula=h(x))

的概率，不是就是另外一个。

### 目标函数的化简

最大似然函数，常规操作，给定了数据集找到符合这个数据集的分布一般也就是最大似然函数适合了。所以![L(\theta) = \prod_{i=1}^mP(y_i|x_i;\theta) = \prod_{i=1}^{m}(h(x_i)^{y_i}(1-h(x_i))^{1-y_i})](https://math.jianshu.com/math?formula=L(%5Ctheta)%20%3D%20%5Cprod_%7Bi%3D1%7D%5EmP(y_i%7Cx_i%3B%5Ctheta)%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D(h(x_i)%5E%7By_i%7D(1-h(x_i))%5E%7B1-y_i%7D))
log化：
![l(\theta) = \sum_{i=1}^m(y_llogh(x_i)+(1-y_i)log(1-h(x_i)))](https://math.jianshu.com/math?formula=l(%5Ctheta)%20%3D%20%5Csum_%7Bi%3D1%7D%5Em(y_llogh(x_i)%2B(1-y_i)log(1-h(x_i))))

![img](https://upload-images.jianshu.io/upload_images/10624272-e15d7bd50bba36c2.png)

所以梯度更新公式：

![\theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h(x_i)-y_i)x_i^j](https://math.jianshu.com/math?formula=%5Ctheta_j%20%3D%20%5Ctheta_j%20-%20%5Calpha%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em(h(x_i)-y_i)x_i%5Ej)

这个方法就梯度下降，问题来了，为什么要用梯度下降？为什么不可以直接等于0呢？

> > #### logistics regression没有解析解
>
> 如果等于0，就有：![\sum_{n=1}^Nsigmoid(\theta^Tx) = x^Ty](https://math.jianshu.com/math?formula=%5Csum_%7Bn%3D1%7D%5ENsigmoid(%5Ctheta%5ETx)%20%3D%20x%5ETy)考察一下两个特征两个样本的情况：
>
> ![img](https://upload-images.jianshu.io/upload_images/10624272-10a2777a8d12e987.png)
>
> 
>
> 有三个不同的sigmoid函数，两个式子解不了，因为sigmoid函数不是线性的，如果是线性的，那么可以吧![\theta](https://math.jianshu.com/math?formula=%5Ctheta)提出来，这样就有解了。其实如果sigmoid去掉，仅仅把![sigmoid(\theta^Tx) = \theta^Tx](https://math.jianshu.com/math?formula=sigmoid(%5Ctheta%5ETx)%20%3D%20%5Ctheta%5ETx)那么就和linear regression一样了。所以是没有解析解的，主要的原因就是因为sigmoid函数是一个非线性的函数。
>
> 





。。。。。。未完待续

# svm与logistic regression的区别

```
两种方法都是常见的分类算法,从目标函数来看,区别在于逻辑回归采用的是logistical loss,svm采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重,减少与分类关系较小的数据点的权重.SVM的处理方法是只考虑support vectors,也就是和分类最相关的少数点,去学习分类器.而逻辑回归通过非线性映射,大大减小了离分类平面较远的点的权重,相对提升了与分类最相关的数据点的权重.两者的根本目的都是一样的.此外,根据需要,两个方法都可以增加不同的正则化项,如l1,l2等等.所以在很多实验中,两种算法的结果是很接近的.
但是逻辑回归相对来说模型更简单,好理解,实现起来,特别是大规模线性分类时比较方便.而SVM的理解和优化相对来说复杂一些.但是SVM的理论基础更加牢固,有一套结构化风险最小化的理论基础,虽然一般使用的人不太会去关注.还有很重要的一点,SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算
svm 更多的属于非参数模型,而logistic regression 是参数模型,本质不同.其区别就可以参考参数模型和非参模型的区别就好了.
logic 能做的 svm能做,但可能在准确率上有问题,svm能做的logic有的做不了
```

另外，SVM在数据分析中会大量内存消耗，速度并不算快，随着数量增加复杂度显著提高。平方数据量的关系？SVM虽然在大数据中应用不佳，但是小量数据中，拥有非常好的范化能力。可由前面的SVM的原理看出









