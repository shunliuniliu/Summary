# 集成学习总结、分类和区别

弱学习算法：识别错误率小于1/2(即准确率仅比随机猜测略高的算法) 

强学习算法：识别准确率很高并能在多项式时间内完成的算法

根据这两个概念，后来产生了一个重要的结论： 

强可学习与弱可学习是等价的，即：一个概念是强可学习的充要条件是这个概念是弱可学习的。

据此，为了得到一个优秀的强学习模型，我们可以将多个简单的弱学习模型“提升”。

对于集成学习，我们面临两个主要问题： 

1.如何改变数据的分布或权重 

2.如何将多个弱分类器组合成一个强分类器

针对上述问题，目前主流方法有三种： 

1.Boosting方法：包括Adaboosting，提升树（代表是GBDT）, XGBoost等 

2.Bagging方法：典型的是随机森林 

3.Stacking算法

**总结一点：bagging和stacking中的基本模型须为强模型（低偏差高方差），boosting中的基本模型为弱模型（低方差高偏差）**

强模型：低偏差高方差

弱模型：低方差高偏差

## 1.1定义

所谓集成学习（ensemble learning），是指通过构建多个弱学习器，然后结合为一个强学习器来完成分类任务。并相较于弱分类器而言，进一步提升结果的准确率。严格来说，**集成学习并不算是一种分类器，而是一种学习器结合的方法**。

集成学习的整个流程：

首次按产生一组“个体学习器”，**这些个体学习器可以是同质的**（homogeneous）（例如全部是决策树），这一类学习器被称为基学习器（base learner），相应的学习算法称为“基学习算法”；集成**也可包含不同类型的个体学习器**（**例如同时包含决策树和神经网络**），这一类学习器被称为“组件学习器”（component learner）。

集成学习通过将多个学习器进行结合，可获得比单一学习器显著优越的泛化性能，它基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好，直观一点理解，就是我们平时所说的“三个臭皮匠，顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。

1.2集成学习的条件
--------------------- 

当然，这种通过集成学习来提高学习器（这里特指分类器）的整体泛化能力也是有条件的：

- 首先，分类器之间应该具有差异性，即要有“多样性”。很容易理解，如果使用的是同一个分类器，那么集成起来的分类结果是不会有变化的。
- 其次，每个个体分类器的分类精度必须大于0.5，如果小于0.5，那么随着集成规模的增加，分类精度会下降；但如果是大于0.5的话，那么最后最终分类精度是可以趋于1的。

因此，要获得好的集成，个体学习器应该“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。

## 1.3 集成学习的分类

根据个体学习器的生成方式，目前集成学习方法大致可分为两大类：

第一类是**个体学习器之间存在强依赖关系、必须串行生成的序列化方法**，这种方法的**代表是“Boosting”；**

第二类是**个体学习器间不存在强依赖关系、可同时生成的并行化方法**，它的**代表是“Bagging”和“Random Forest”**；

**Bagging**：通过对原数据进行有放回的抽取，构建出多个样本数据集，然后用这些新的数据集训练多个分类器。因为是有放回的采用，所以一些样本可能会出现多次，而其他样本会被忽略。

该方法是通过降低基分类器方法来改善泛化能力，因此**Bagging的性能依赖于基分类器的稳定性**，如果基分类器是**不稳定的**，Bagging有助于**减低训练数据的随机扰动导致的误差**，但是如果基分类器是**稳定**的，**即对数据变化不敏感**，那么Bagging方法就**得不到性能的提升，甚至会降低**。

**Boosting**：提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用（对错分数据进行惩罚）。



### Bagging与Boosting的区别：

- 二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。
- Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；
- Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。
- **bagging是减少variance，而boosting是减少bias**。
- Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap)然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。
- Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行。



### gradient boosting

gradient boosting：Boosting是一种思想，**Gradient Boosting是一种实现Boosting的方法**，

它主要的思想是，

每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。



### RandomForest与bagging的区别：

1）Rand forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本；

2）bagging是用全部特征来得到分类器，而rand forest是需要从全部特征中选取其中的一部分来训练得到分类器；

3）Random Forest属于Bagging的扩展变体

4）其实**随机森林是对装袋法的一种改进**，随机森林也需要对自助抽样训练集建立一系列的决策树，这和决策树类似。不过，随机森林在**建立树的时候**，不和装袋法一样，**装袋法建树的时候是将所有预测变量都考虑进去，而随机森林则是考虑每一个分裂点时，都是从所有的预测变量p中随机选取m个预测变量，分裂点所用的预测变量只能从这m个变量中选择**。在每**个分裂点处都重新进行抽样，选出m个预测变量，通常m≈√p**，对每一个分裂点来说，这个算法将大部分可用预测变量排除在外，虽然听起来crazy,但是这个原理是很巧妙的。

其实**当随机森林中的m=p时，随机森林和装袋法是一样的**。随机森林考虑每个分裂点的子集相对来说比装袋法少很多。这样得到的树的平均值有更小的方差，因而树的可信度相对来说比较高。

一般Rand forest效果比bagging效果好！

#### random forest改进了bagging的什么不足之处？（改进方法再上面的（4）中）

仔细思考bagging下会出现一些新的问题，如果一个数据集有一个很强的预测变量和一些中等强度的预测变量，那么可以想到，大多数（甚至所有）的树都会将最强的预测变量用于顶部分裂点，**这会造成所有的装袋法树看起来都很相似**。与不相关的量求平均相比，对许多高度相关的量求平均带来的方差减小程度是无法与前者相提并论的。在这种情况下，**装袋法与单棵树相比不会带来方差的重大降低。这个问题是装袋法一个很致命的问题**。那么下面我们来看看random forests method. 



## Bagging：

并行式学习方法的著名代表。**主要关注降低方差variance。**

  (1）从样本集中重采样（有重复的）选出n个样本；

（2）在所有属性上，对这n个样本建立分类器（ID3、C4.5、CART、SVM、Logistic回归等）；

（3）重复以上两步m次，即获得了m个分类器；

（4）将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类。

疑问1：n的值如何选择？

疑问2：m的值如何选择？——选择奇数个分类器即可。

注：**与其将Bagging理解为一个算法，不如将其理解为一种思想，即综合多个弱分类器的结果得到一个强分类器的思想！**

样本扰动：

直接基于自助采样法（Bootstrap Sampling），使得初始训练集中约63.2%的样本出现在一个采样集中（剩余的36.8%可用作验证集来对泛化性能进行“OOB包外估计” "Out of Bag estimation"）。根据多个采样集训练多个个体学习器。预测时采用简单投票或者简单平均法。

补充：

包外估计其他用途：基学习器为决策树时，可以用OOB作为验证集来辅助剪枝，或用于估计决策树中各节点的后验概率以辅助队零训练样本节点的处理；当基学习器为神经网络时，可使用OOB辅助Early Stopping减小过拟合风险。

训练一个Bagging与直接使用个体学习器算法训练一个学习器的复杂度同阶，说明Bagging是一个很高效的集成算法，另外与标准Adaboost只适用于二分类任务不同，bagging能不经修改就直接用于多分类、回归任务。



## RandomForest：

**属于Bagging的扩展变体。随机森林以CART决策树为基学习器。**

随机森林是一个用随机方式建立的，包含多个决策树的集成分类器。其输出的类别由各个树投票而定（如果是回归树则取平均）。假设样本总数为n，每个样本的特征数为a，则随机森林的生成过程如下：

1. 自助采样（Bootstrap Sampling）：从原始样本中采用有放回抽样的方法选取n个样本；
2. 对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点；
3. 重复m次，获得m个CART决策树；
4. 对输入样例进行预测时，每个子树都产生一个结果，采用简单投票机制或者简单平均输出。



> 在整个随机森林算法的过程中，有**两个随机过程**：
>
> 1.输入数据是随机的：从全体训练数据中选取一部分来构建一棵决策树，并且是有放回的选取 
> 2.每棵决策树的构建所需的特征是从全体特征中随机选取的
>
> 这两个随机过程使得随机森林在很大程度上避免了过拟合的出现。

**即**

随机森林的随机性主要体现在两个方面：

**样本扰动：**直接基于自助采样法（Bootstrap Sampling），使得初始训练集中约63.2%的样本出现在一个采样集中（剩余的36.8%可用作验证集来对泛化性能进行“OOB包外估计” "Out of Bag estimation"）。根据多个采样集训练多个个体学习器。预测时采用简单投票或者简单平均法。

**属性扰动：**随机选择特征属性进行节点划分（传统决策树中，会在当前结点的特征属集合中选择一个最优属性进行划分，而在RF中，对基决策树的每个结点，先在该结点的特征属性集合中随机选择k个属性，然后再从这k个属性中选择一个最优属性进行划分，一般推荐k=log2d）

**随机森林为什么性能强大？**

随机森林对Bagging只做了小改动，但与Bagging中基学习器的“多样性”仅通过样本扰动不同的是，随机森林不但通过样本扰动，还引入了属性扰动，这就使得最终的泛化性能可通过基学习器之间的差异度增加而进一步提升。即决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。

**随机森林的优点：**

实现简单，训练速度快，泛化能力强，可以并行实现（因为训练时树与树之间是相互独立的）；

相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；

能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；

对于不平衡的数据集，可以平衡误差；

相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；

训练完成后可以给出哪些特征比较重要，即可以用于特征筛选。

**随机森林的缺点：**

在噪声过大的分类和回归问题还是容易过拟合；

相比于单一决策树，它的随机性让我们难以对模型进行解释。



## Boosting

是一族可将若学习器提升为强学习器的算法。**主要关注降低偏差**。

Boosting方法是一种典型的基于bootstrapping思想的应用，其特点是：每一次迭代时训练集的选择与前面各轮的学习结果有关，而且每次是通过更新各个样本权重的方式来改变数据分布。总结起来如下：

1.分步学习每个弱分类器，最终的强分类器由分步产生的分类器组合而成 

2.根据每步学习到的分类器去改变各个样本的权重（被错分的样本权重加大，反之减小)

Boosting方法和Bagging类似，**与其将其理解为一个算法，不如将其理解为一类算法的思想。**即：通过m次的迭代，每次迭代训练出不同的弱分类器，然后将这m个弱分类器进行组合，形成一个强分类器。Adaboost就是这类算法中最具代表性的一个算法。

工作机制：先从初始训练集中训练处一个基学习器，再根据基学习器的表现对训练样本分布进行调整，**使得先前基学习器做错的样本在后续收到更多关注**，然后**基于调整后的样本分布再训练下一个基学习器**，如此重复进行，**直至基学习器数目达到实现指定的数目T**，

**最终将这T个基学习器进行加权结合。**（加权的系数就是最终不断调整好的数组中的权重）

常见模型：**Adaboost、GBDT、XGBoost**



### GBDT（Gradient Boost Decision Tree 梯度提升决策树）

GBDT是以决策树为基学习器的迭代算法，注意GBDT里的决策树都是**回归树**而不是分类树。Boost是”提升”的意思，**一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果**。 

GBDT核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。

比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。 

**GBDT优点：**

是适用面广，离散或连续的数据都可以处理，几乎可用于所有回归问题（线性/非线性），亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。

**GBDT缺点：**

是由于弱分类器的串行依赖，导致难以并行训练数据。



### 随机森林和GBDT的区别：

1. 训练集选取：随机森林采用的Bagging思想，而GBDT采用的Boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的；
2. 决策树类型：**组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成；**
3. 结果预测：对于最终的输出结果而言，随机森林采用多数投票、简单平均等；而GBDT则是将所有结果累加起来，或者加权累加起来；
4. 并行/串行：组成随机森林的树可以并行生成；而GBDT只能是串行生成；
5. 异常值：随机森林对异常值不敏感；GBDT对异常值非常敏感；
6. 方差/偏差：随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。





### Adaboosting：

Adaboosting（adaptive boosting）算法由Schapire于1996年提出，是具体实现boosting方法的最典型代表。

算法过程简述为：前一个分类器改变权重w后，成为后一个新的分类器。 **如果一个训练样本在前一个分类器中被误分，那么它的权重会被加大，相反，被正确分类的样本权重会降低。**

即：AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得之前的分类模型出错的训练样本在后续受到更多关注的方式来弥补已有模型的不足。通过每一次迭代改变训练数据的权值分布，使得数据在每一个基本分类器的学习中起到不同作用，从而使不同数据发挥各自不同的作用，因此不易发生过拟合。

相关的训练误差分析表明，每一次迭代后构建的分类器，其分类误差率随着迭代次数增加而稳定下降。



AdaBoost算法实际上是一种加法模型，损失函数为指数函数，学习算法为前向分步算法。

AdaBoost的主要优点如下： 

1.是一种有高精度的分类器 

2.可以使用各种方法构建子分类器，adaboost算法提供的是一种框架 

3.基本分类器构建简单，易于理解 

4.算法过程简单，不用做特征筛选 

5.不易发生过拟合



## Stacking

stacking是一种组合多个模型的方法，它主要注重分类器的组合，相对于bagging和boosting使用较少，具体过程如下：

1.**划分训练数据集为两个不相交的集合。** 

2.在第一个集合上**训练**多个学习器。 

3.在第二个集合上**测试**这几个学习器 

4.将第三步得到的预测结果作为输入，将正确的映射作为输出，训练一个更高层的分类器

由此可见，对于模型组合方式，不同于前面两个方法采用多数投票或算术平均的线性组合策略，stacking采用的是基本模型非线性组合的方式。



