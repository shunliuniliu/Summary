# 2018.12.11

## 今日完成进度

- 已完成Anaconda、Git、Pycharm、DataGrip、Sublime Text、Typora等开发与办公软件的安装
- 熟悉python-pptx模块操作ppt，并完成共250+行的2个使用Demo，整理出文档

## 明天任务

- ~~等待南方通帐号激活~~
- ~~安装wind，连接数据库~~
- ~~接收周总的[指数投资]的资料，并完成学习~~



# 2018.12.12

## 今日进度

- 安装Mysql
- 熟悉python操作excel和word的模块，并完成多个使用Demo，整理出文档
- 配置安装PLSQL
- 学习ETF的ppt资料
- 开通wind、南方通
- **接受周总的沪港通、深刚通、陆港通任务**。加上**量化研究任务**，预计两三周内完成
  1. 先爬取港交所的数据到数据库中，
  2. 等wind的代码生成器可用后，把陆港通数据存储到数据库中
  3. 先完成数据采集的任务，再和周总商量比对方案、

## 第一阶段任务

- 完成可视化爬虫系统
- 完成港交所官网数据和wind数据的比对验证

## 明天任务

- ~~把金融大数据处理、fluent python、量化投资这几本书带过来~~

- [~~先爬取港交所的数据到数据库中~~](https://sc.hkex.com.hk/TuniS/www.hkex.com.hk/Mutual-Market/Stock-Connect/Statistics/Historical-Daily?sc_lang=zh-CN#select4=0&select5=0&select3=0&select2=11&select1=11)

- **~~等wind的代码生成器可用后，把陆港通数据存储到数据库中~~**，直接连接wind底层oracle数据库即可，速度更快且没有数据限制。

### 方案（验证可行）

Gerapy、Scrapy、~~查找及借鉴github现有项目~~、~~试一下八爪鱼~~

PLSQL Developer还需要配置python接口，直接连公司买的数据库，没有数据量的限制，wind的python接口会有数据量的限制

# 2018.12.13

## 今日进度

- 确定Gerapy、Scrapy的方案，已确定使用了Gerapy、Scrapy、Scrapyd，还使用了Scrapydweb
- 完成了一个完整的爬虫，总结遇到的各类问题，见文档**[2018.12.13-Gerapy和Scrapyd 部署Scrapy项目-Scrapydweb和Scrapyd 部署Scrapy项目]**
- ~~[安装谷歌浏览器插件SelectorGadge](https://www.jianshu.com/p/3465b700b11f)，简化【选择网页中节点】~~（项目中暂时不需要）可以简化自己写选择器逻辑

## 明天任务

- ~~把金融大数据处理、fluent python、量化投资这几本书带过来~~
- ~~等wind的代码生成器可用后，把陆港通数据存储到数据库中。或者通过PLSQL连接底层数据库~~。直接连接wind底层oracle数据库即可，速度更快其没有数据限制。
- ~~询问周总，两个来源的数据比较哪些点。~~



# 2018.12.14

## 今日进度

- 完成Gerapy、Scrapy港交所官网爬虫的2.0版本，进一步优化了插入数据库的操作。
  1. 沉淀出一个可视化爬虫系统——针对港交所的深港通沪港通的每日数据
  2. 沉淀出一个Oracle和Mysql数据库之间互导数据的脚本工具
- PLSQL读取【陆港通前十大成交活跃股】SHSCTOP10ACTIVESTOCKS的数据，把数据从Oracle导到Mysql中。完成港交所官网数据和wind数据的比对。结论：比对之后，港交所官网上的数据在wind中都有，且是wind数据库的子集。

## 第二阶段任务

目的：

- 分析出香港投资者及内地投资者的投资偏好，与市场影响直接的关系
- 分析与沪深港三地股市的涨跌趋势的关系

执行途径：

- 通过wind寻找【分类】数据，各种类别的信息，比如“持仓”，“陆港通卖空数据”等
- 分析“北上”、“南下”的资金历史走向、趋势变化等，频率包括每月、每周、每日
- 看研报，找找思路
- 事件影响。事件发生日的市场变化，对市场的近期长期的影响

# 2018.12.17

## 第二阶段任务补充：

探索使用DM、ML中的算法，比如关联规则挖掘两地股市的数据关联。

## 今日进度： 

- 阅读研究报告、论文探索思路——整理出[参考资料]、[步骤及算法]、[软件工具]，安装相关的数据分析软件和模块。

- 积累相关概念

## 明天任务：

优先级一：

- 实现至少一篇论文的步骤及算法，验证结论
- 再此基础上，探索下自己的创新思路

优先级二：

- 针对事件驱动，考虑训练下自然语言处理，进行分类打分的预测模型
- 思考下下DM、ML中的回归、分类算法，哪些可以运用。

# 2018.12.18

## 今日进度： 

- 论文[深港联动性分析]的实证完成70% （**后续继续完成**）

## 明天任务：

- ~~完成中期答辩的任务的第一项~~


# 2018.12.19

## 今日进度： 

- [中期答辩任务]完成第二项，代码可以复用与第1、4、5项

- 请教石赟凯关于指数与流通市值等的关系逻辑（**后续查资料，整理成文档**）

## 明天任务：

- **~~存在一个问题：SHSCMembers中存在的都是entry 和 remove信息，并不是每个交易日对应的沪股通的所有成分股，需要自己再做进一步处理找wind经理找对应的数据接口~~** 结论是：wind只能这样提供

- 完成[中期答辩任务]完成第1、4项，注意流通市值的数据表是否正确，找wind经理确认

- ~~找wind经理要第5项对应的数据接口或者数据库表~~

- ~~中期答辩~~


# 2018.12.20

## 今日进度： 

- ~~完成答辩~~

## 明天任务：

- ~~**存在一个问题：SHSCMembers中存在的都是entry 和 remove信息，并不是每个交易日对应的沪股通的所有成分股，需要自己再做进一步处理找wind经理找对应的数据接口**~~ 结论是：wind只能这样提供

- 完成[中期答辩任务]完成第1、4项，注意流通市值的数据表是否正确，找wind经理确认

- ~~找wind经理要第5项对应的数据接口或者数据库表~~



# 2018.12.21

## 第三阶段任务：

### 一、监控陆股通、港股通标的，一共是沪深港三个市场中的标的监控。

1. 包括entry、remove信息
2. 开发爬虫前，先搞清三方面的情况
   - 陆港通标的
     1. 入选和剔出规则
     2. 公布规则
   - 陆港通临时调整的规则
   - 如何公布标的，公布规则
3. 开发爬虫

### 二、先做A股市场的股本监控

前提：如果T日公布股本变动的情况，T+1日就立即生效，进行调整，则去上交所官网取爬取数据，因为wind有延时；如果是T+3日之后才生效调整，则从wind数据库获取数据即可。

1. 先看上交所官网相关公告，先总结，包括但不限于：公告时间规则
2. 监控的数据包括：股本的变动情况，原因等

## 今日进度： 

- ~~梳理任务及方案~~

## 明天任务：

- 


# 2018.12.24

## 今日进度：

- ~~完成[第三阶段任务一]监控陆股通、港股通标的，一共是沪深港三个市场中的标的监控。~~
  1. ~~入选、剔出、公布等规则文件~~
  2. ~~爬虫代码~~
  3. ~~暂时以excel文件保存，还没完成想好应该按照什么形式展示~~ 
- ~~邮件与wind后台沟通数据表~~

## 明天任务：

- ~~完成[第三阶段任务二] A股市场的股本监控~~
- ~~完成邮件与wind后台沟通数据表~~
- ~~MSCI成份股与港股通调整信息整合~~




# 2018.12.25

## 今日进度：

- ~~完成[第三阶段任务一]监控陆股通、港股通标的，一共是沪深港三个市场中的标的监控。~~
  1. ~~入选、剔出、公布等规则文件~~
  2. ~~爬虫代码~~
  3. ~~MSCI成份股与港股通调整信息整合，并按照肖磊哥要求，输出excel~~
- ~~邮件与wind后台沟通数据表，得到准确的数据表信息~~
- ~~A股市场股本变动监控，完成爬虫脚本。与wind后台沟通了数据表~~

## 明天任务：

- ~~A股市场股本变动监控，完成爬虫脚本，与wind后台沟通了数据表。~~ 明天与周总商量具体以哪个为准。

- ~~完成[中期答辩任务]完成第2项，注意：之前遇到的“沪股通名单”问题，利用wind数据表，自己进行计算出：某一交易日的所有有效的沪股通成份股~~

- 完成[中期答辩任务]完成第1、4项，注意流通市值的数据表是否正确


# 2018.12.26

## 今日进度：

- ~~完成[第三阶段任务二]A股市场股本变动监~~
- ~~完成[中期答辩任务]完成第2项~~

## 明天任务：

- 完成[中期答辩任务]完成第4、1、5项，注意流通市值的数据表是否正确


# 2018.12.27

## 今日进度：

- ~~完成[中期答辩任务]完成第4项~~

## 明天任务：

- ~~把[中期答辩任务]完成第4项代码从github上拉下来，在本地进行部署，保证移植的可靠性~~
- ~~中期答辩任务]完成第2项，（1）增加沪深300，要求展示沪深300和中证500；（2）均要增加可视化折线图~~
- ~~完成[中期答辩任务]完成第1、5项，注意流通市值的数据表是否正确~~



# 2018.12.28

## 今日进度：

- ~~中期答辩任务]完成第1项~~
- ~~中期答辩任务]完成第5项，做完预备工作~~

## 明天任务：

- ~~完成中期答辩任务]完成第5项(公司断电，明天无法完成，只能后天和大后天，来公司做完)~~
- ~~看[养老宝-养老金策略，定投相关策略]~~
- ~~搜索Django做网站的project，更新以前的思路。确定一个合适的方案。~~



# 2018.12.29

## 今日进度：

- 发现有新的项目汇总，mark以下，方便以后空闲时候学习下别人的项目

- [32个Python爬虫项目](https://zhuanlan.zhihu.com/p/27938007)

- [70个Python练手项目列表](https://zhuanlan.zhihu.com/p/27931879)

- 指数增强组-展示网站：

  1. 技术方案（多种）：

     - 整体：Vue.js + uWsgi + Nginx + **Django** + ~~MySQL~~  **Oracle**+ Ubuntu [参考1](https://www.cnblogs.com/jieru/p/7144707.html) [参考2](http://www.geerniya.cn/)

       - [ ] 对Vue不太熟悉，建议换成**Angular+Bootstrap**,bootsreap默认样式太单调，用 **[bootswatch**](https://bootswatch.com/)可以提供不同的主题**(推荐尝试)**
       - [ ] 考虑避免以后的麻烦，使用virtualenv**（暂时可以不需要）**
       - [ ] 构建日志服务 **（暂时可以不需要）（[很简单，配置下、引入模块、代码中写logging即可](https://www.cnblogs.com/jieru/p/7144707.html)）**

     - 后端的数据计算：TA-Lib 、pandas、[pyfolio](https://github.com/quantopian/pyfolio)、[empyrical](https://github.com/quantopian/empyrical)  **(手动计算)**

     - 图表展示：**Pyecharts**(官网有在Django使用的示例)，[Plotly](https://github.com/plotly/plotly.py)，[TuChart](https://github.com/Seedarchangel/TuChart)Tushare和Echarts结合，针对中国股市做常见数据的可视化（可以借鉴该项目的开发思路，如何把代码结构写的优雅）[小型app一样](https://github.com/Seedarchangel/TuChart/blob/master/Example_Graphs/En_US.md)

     - git+Pycharm(使用内置的)做版本控制（**在本地搭建git仓库**）

     - [DJANGO CELERY 定时任务](https://www.cnblogs.com/wumingxiaoyao/p/8521285.html)在Django中配置定时任务很有用（**需要定时任务，但是在代码中实现还是Django框架中实现，看具体难度而定**）

     - [基于Django+celery二次开发动态配置定时任务](https://www.cnblogs.com/huangxiaoxue/p/7266253.html),很适用后期需求多的情况

  2. 可能用到的模块：

     - [Json-to-HTML-Table](https://github.com/afshinm/Json-to-HTML-Table)
     - [ HTML-Table-to-JSON](https://github.com/tremblay/HTML-Table-to-JSON)

  3. 代码检查：

     - Pycharm自带的 Inspect Code（代码静态审查），检查出不符合PEP8规范的地方
     - PyChecker模块可以检查出项目中的bug等

  4. 备选方案考虑: [wordpress搭建网站](https://www.jianshu.com/p/e017a2ea7991?utm_source=oschina-app)，但是有个弊端，就是后续在后台整合python的各种小工具时，无法兼容。**（不适用）**

  5. 消息通知，除了利用公司已有的邮件、短信，还可以考虑用微信通知。**（暂不适用）**

  6. 汇报展示：除了ppt，还有网页版展示[**nodeppt**](https://github.com/ksky521/nodeppt)可以考虑(目前见过的最强大方便的网页版presentation工具，没有之一)

## 明天任务：

- ~~完成中期答辩任务]完成第5项(公司断电，明天无法完成，只能后天和大后天，来公司做完)~~
- ~~看[养老宝-养老金策略，定投相关策略]~~
- ~~搜索Django做网站的project，更新以前的思路。确定一个合适的方案。~~



# 2018.12.30

## 今日进度：

- ~~完成中期答辩任务]完成第5项~~

## 明天任务：

- ~~看[**养老宝-养老金策略(重点)**，定投相关策略]~~
- ~~找Django网站的现有的优秀project~~


# 2019.1.1

## 今日进度：

- [Python运维中20个常用的库和模块](https://zhuanlan.zhihu.com/p/52947756)，重点标出
  1. psutil是一个跨平台库。获取系统运行的进程和系统利用率（内存，CPU,磁盘，网络等），主要用于系统监控，分析和系统资源及进程的管理。
  2. difflib：difflib作为Python的标准模块，无需安装，作用是对比文本之间的差异。
  3. filecmp:系统自带，可以实现文件，目录，遍历子目录的差异，对比功能。
  4. **scapy**([http://www.wecdev.org/projects/scapy/](http://link.zhihu.com/?target=http%3A//www.wecdev.org/projects/scapy/))是一个强大的交互式数据包处理程序，它能够对数据包进行伪造或解包，包括发送数据包，包嗅探，应答和反馈等功能。
  5. playbook：一个非常简单的配置管理和多主机部署系统。
- 基金相关概念

## 明天任务：

- 看[**养老宝-养老金策略(重点)**，定投相关策略]
- ~~找Django网站的现有的优秀project~~



# 2019.1.2

# 第四阶段任务：

- 流程自动化工具，定时任务调度平台。[Django自带的用户身份验证](https://docs.djangoproject.com/zh-hans/2.0/topics/auth/)
- 期货基差

## 今日进度：

- ~~熟悉Django，搭建demo~~
- ~~新增A股股本变动比率：changevol除以totalshare~~
- ~~前十大成交活跃股，做降序排列~~
- 查找“任务调度”、“定时任务”、“可视化的调度”、流程自动化工具或平台(Python)
  1. 轻量级-APScheduler：[参考1 ](https://www.jianshu.com/p/ad2c42245906)、[参考2](https://www.cnblogs.com/domestique/p/7814007.html)、[参考3](https://www.cnblogs.com/alexzu/p/8661909.html)、[同一时刻执行多个定时任务](https://zhuanlan.zhihu.com/p/38427932)、[**flask-apscheduler**](https://github.com/viniciuschiele/flask-apscheduler)、
  2. 用python-crontab实现定时任务（django-crontab实现定时任务：安装django-crontab）[参考1](https://blog.csdn.net/python_tty/article/details/51729569)
  3. 重量级-celery-beat（可结合Django）：[参考1](https://www.cnblogs.com/jiangshanwang/p/9146227.html)、[django celery](https://zhuanlan.zhihu.com/p/30742764)
  4. **自动化运维工具Ansible，自带web界面；带有定时任务模块** （用python做运维，这一个工具足够了！！！）
  5. **Github上可借鉴项目，见star**
- ~~先看flask-apscheduler~~

## 明天任务：

- ~~展示网站第一版~~
- ~~定时任务web工具调通一个demo~~
  1. ~~Github上项目~~
  2. ~~自己用schedule写个简单的~~

# 2019.1.3

## 今日进度：

- ~~孙总指派任务：（1）"成份股处罚"(wind、披露易、证监会官网)（2）深圳创新(根据1.体量 2.行业代表性，wind一级行业即可 3.超过50%控股的大股东，排除国资委 。选出的成份股行业的分布尽量均匀)~~
- 查"归因"、“施密特正交"

## 明天任务：

- ~~展示网站第一版~~
- ~~定时任务web工具调通一个demo~~
  1. ~~Github上项目~~
  2. ~~自己用schedule写个简单的~~




# 2019.1.7

## 今日进度：

- 展示网站修正（按崔总要求）
- 任务调度的调研和周总需求，都已搞清楚。申请完服务器后，开始搭建。
- **结论：验证了**
  1. **ansible太大，不适合这次需求**
  2. **Django celery定时任务功能OK，但是重启等功能和界面不符合**
  3. **Django apschedule定时任务功能OK，但是重启等功能和界面不符合**
- 需要在linux搭建今天调研的project

## 明天任务：

- 做出任务调度平台第一版

- 告诉jiashi：talib有导出json格式的方法

  ````
  >>> print(data.export('json'))
  [
    {
      "last_name": "Adams",
      "age": 90,
      "first_name": "John"
    },
    {
      "last_name": "Ford",
      "age": 83,
      "first_name": "Henry"
    }
  ]
  ````



# 2019.1.8

## 今日进度：

- 完成凯宁哥交待的任务
- 搭建服务器环境

## 明天任务：

- 做出任务调度平台第一版

# 2019.1.9

## 今日进度：

- nginx+gunicorn+supervisor+django部署-探索尝试（完成50%）
- CTask项目搭建（error）
- OpenMangosteen搭建（error）
- Django+Xadmin+celery构造定时任务项目（success）

## 明天任务：

- ​

# 2019.1.10

## 今日进度：

- 完成指数增强-展示网站version2，添加很多新功能

参考资料：

1. [bootstrap table和tableExport导出支持中文的Excel和pdf等表格](https://blog.csdn.net/youand_me/article/details/76642434)
2. [bootstrap-table官网](https://bootstrap-table-examples.wenzhixin.net.cn/index.html?view-source#extensions/export.html)

- 完成定时任务平台初步展示界面

## 明天任务：

- ~~在threegrid表格上加上导出下载文件的功能！！！~~
- 完成dev_task项目搭建，和作者蔡青沟通
- 完成定时任务平台的（1）后端逻辑（2）基本任务添加调度



# 2019.1.11

## 今日进度：

- [Ovirt](https://yq.aliyun.com/ziliao/335135)、[Android App 测试 Appium、Robotium、monkey对比](https://blog.csdn.net/dubo_csdn/article/details/81809705)、app测试网课


- 完成threegrid表格上加上导出下载文件的功能，及表格刷新功能
- Dev_task部署成功，但是uwsgi还未和nginx对接成功
- RabbitMQ:15672
- nginx:8070
- uwsgi:3400
- 参考资料：[使用supervisor支持Python3程序](https://www.cnblogs.com/andy-0212/p/9999639.html)

## 明天任务：

- 论文任务
- dev_task跑通

# 2019.1.12

## 今日进度：

- 总结connection reset by peer的可能原因


- 完成【部署Django+Nginx+Gunicorn】，并分析出每一环节报错的原因。

**[部署Django+Nginx+Gunicorn]**

https://www.cnblogs.com/nanrou/p/7026802.html

https://www.cnblogs.com/gaidy/p/9784919.html

```
gunicorn -w 3 -b 127.0.0.1:8080 project.wsgi:application
```

也可以将gunicorn启动配置化

如果gunicorn没有开reload功能，那么在改django代码之后要手动重启gunicorn。

- 【部署Django+Nginx+uwsgi】也成功

[Django 搭建单服务实现多域名访问](https://mp.weixin.qq.com/s/dDIOlTT2BYupgECCf3dS6Q)

[uwsgi使用](https://www.jianshu.com/p/c3b13b5ad3d7)

**目前部署成功的uwsgi.ini文件**

```
[uwsgi]
# 项目目录
chdir = /home/zhangjinzhi/projects/venv_timed_task/dev_task
# 指定项目的application
wsgi-file = %(chdir)/dev_task/wsgi.py
# 指定sock的文件路径
socket=%(chdir)/logs/uwsgi.sock
# 进程个数
workers=1
pidfile=%(chdir)/pid/uwsgi.pid
# 指定IP端口
# nginx负载均衡使用socket,uwsgi启动服务使用http
#socket=192.168.2.200:8000
http=127.0.0.1:3400
# 启用主进程
master=true
# 自动移除unix Socket和pid文件当服务停止的时候
vacuum=true
# 序列化接受的内容，如果可能的话
thunder-lock=true
# 启用线程
enable-threads=true
# 设置自中断时间
harakiri=30
# 设置缓冲
post-buffering=4096
# 设置日志目录
daemonize=%(chdir)/logs/uwsgi.log
```

- django + gunicorn + nginx 关系剖析

对django + gunicorn + nginx 这三兄弟的理解。首先我们知道，我们访问网站，就是去网络上的一台电脑里访问某个路径下的某个文件，那django的作用主要是做（生产）这个文件，拿一家餐馆来讲，我认为django就是这个餐馆的厨师，他负责做菜，当规模很小的时候，比如路边卖鸡蛋饼的大妈，因为客人不多，所以可以自己问客人要什么，然后再自己做，这就是django和自带的runserver所做的事情；那当规模变大了，比如普通餐馆，客人很多，厨师做菜都来不急了，根本没时间去问客人要什么，所以这个时候我们就需要服务员了，服务员去记录客人要什么，然后跟厨房讲，接着从厨房拿菜给客人，而在这里，gunicorn就是这个服务员；当规模更大一些的时候，每分钟都有几百个人（现实中来讲这已经是多到爆炸了吧）要进餐馆吃饭，你在餐馆里安排再多的服务员也不能处理完这么多客人的请求，而且餐馆的空间是有限的，服务员也占空间，多了放不下，所以这个时候怎么办呢，答案是在餐馆门口安排咨客，有序地引导客人进入餐馆，也可以在门口就帮客人点好菜，提高整体效率，Nginx就扮演了咨客这个角色

- **目前部署成功的conf文件**

```
server{
    listen 8070; # 监听的端口
    server_name 106.13.70.248;
    server_name 127.0.0.1;
    server_name your_www;
    #当请求这些server name的时候，nginx才会做反向代理，0.0.0.0是指全部
    location / {
      proxy_pass http://127.0.0.1:3400;
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }
    # location 顾名思义，定位，就是当访问 / 的时候，nginx会将请求转给本地的8080端口，而后面的设置都是一些基本的配置，可以直接用
    location /static {
      alias /home/zhangjinzhi/projects/venv_timed_task/dev_task/static/;
    }
    # 这个就是配置静态文件的地方，要用绝对地址，对应最开始的目录形式，假设project就在/home下面，那么这样配置就可以的了，还有个前提是，你在开发的时候，采>取了django的建议，每个app的静态文件都用多了一层app_name的文件夹来包住。
}
```

**或者：（也成功，配置项更少）**

```
server {
    listen 80;
    server_name 106.13.70.248; # 这是HOST机器的外部域名，用地址也行

    location / {
        proxy_pass http://127.0.0.1:3400; # 这里是指向 gunicorn host 的服务地址
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

}
```

原先一直不成功的conf文件

```
        server{
            listen  8070;
            # server_name     106.13.70.248;
            charset utf-8;
            location /static {
                    #alias /opt/dev_task/static/;
                     alias /home/zhangjinzhi/projects/venv_timed_task/dev_task/static/;
            }
            location / {
                    include     uwsgi_params;
                    uwsgi_pass 127.0.0.1:3400;
            }
            # access_log /data/log/nginx/dev_task_access.log;
            # error_log /data/log/nginx/dev_task_error.log;
    }
```



# 2019.1.13

## 今日进度：

**注意：！！！用自己设置的虚拟环境时，配置文件中的command，一定改成对应自己的虚拟环境中的命令！！**（见目前的server端superviosr.conf文件）

## 明天任务：

- **在服务器上搭建OpenMangosteen**

# 2019.1.14

## 今日进度：

- 走通了单个rabbitmq和celery任务的逻辑，但是还没和前端打通

### 积累：

- Celery的任务模块Task中包含异步任务和同步任务：

  1. 异步任务：业务逻辑中被处罚并发往任务队列，**可以做成“事件触发”任务**
  2. 定时任务：同之前思路，不变

- from future import absolute__import 是拒绝隐式引入，因为celery.py和名字和celery的包名冲突，需要使用这条语句让程序正确地运行。

- [爬虫架构|Celery+RabbitMQ快速入门（四）整合版本](https://www.jianshu.com/p/e526b6742384)

  - ```python
    # 使用RabbitMQ作为消息代理
    BROKER_URL='amqp://spider:*****@IP:端口/****' 
    # 把任务结果存在了Redis
    CELERY_RESULT_BACKEND = 'redis://localhost:6379/0' 
    # 任务序列化和反序列化使用JSON方案
    CELERY_TASK_SERIALIZER = 'json' 
    # 读取任务结果使用JSON
    CELERY_RESULT_SERIALIZER = 'json' 
    # 任务过期时间，不建议直接写86400，应该让这样的magic数字表述更明显
    CELERY_TASK_RESULT_EXPIRES = 60 * 60 * 24 
    # 指定接受的内容类型，是个数组，可以写多个
    CELERY_ACCEPT_CONTENT = ['json']
    ```

  - ```python
    from kombu import Queue
    CELERY_QUEUES = ( # 定义任务队列
        Queue('default', routing_key='task.#'), # 路由键以“task.”开头的消息都进default队列
        Queue('web_tasks', routing_key='web.#'), # 路由键以“web.”开头的消息都进web_tasks队列
    )
    
    CELERY_DEFAULT_EXCHANGE = 'tasks' # 默认的交换机名字为tasks
    CELERY_DEFAULT_EXCHANGE_TYPE = 'topic' # 默认的交换类型是topic
    CELERY_DEFAULT_ROUTING_KEY = 'task.default' # 默认的路由键是task.default，这个路由键符合上面的default队列
    
    CELERY_ROUTES = {
        'projq.tasks.add': { # tasks.add的消息会进入web_tasks队列
        'queue': 'web_tasks',
        'routing_key': 'web.add',
        }
    }
    ```

- [使用LinkExtractor提取链接](https://www.jianshu.com/p/7c5d41c61ad2)

- Scrapy本身并不提供JS渲染解析的功能，那么如何通过Scrapy爬取动态网站的数据呢？**(重要!后续会很有用)**

  - 模拟接口去获取需要的数据（一般也推荐这种方式，毕竟这种方式的效率最高），但是很多网站的接口隐藏的很深，或者接口的加密非常复杂，导致无法获取到它们的数据接口，此种方法很可能就行不通。
  - 借助JS内核，将获取到的含有JS脚本的页面交由JS内核去渲染，最后将渲染后生成的HTML返回给Scrapy解析，**Splash是Scrapy官方推荐的JS渲染引擎**，它是使用Webkit开发的轻量级无界面浏览器，提供基于HTML接口的JS渲染服务。（Selenium和phantomJs应该也是可以，可能没有Slpash方便）
  - Python库的scrapy-splash是一个非常好的选择，这里有一系列的实例教程**[使用Splash爬取动态页面](https://www.jianshu.com/p/e54a407c8a0a)**

- eval() 函数用来执行一个字符串表达式，并返回表达式的值。

- Django中locals()用法：locals()可以直接将函数中所有的变量全部传给模板。当然这可能会传递一些多余的参数，有点浪费内存的嫌疑。

  ```python
  return render(request, 'blog_add.html', locals())
  return render_to_response('blog_add.html', locals())
  ```

- #### Django中过滤器

  模板过滤器可以在变量被显示前修改它，过滤器使用管道字符，如下所示：

  ```
  {{ name|lower }}
  ```

  {{ name }} 变量被过滤器 lower 处理后，文档大写转换文本为小写。

  过滤管道可以被* 套接* ，既是说，一个过滤器管道的输出又可以作为下一个管道的输入：

  ```
  {{ my_list|first|upper }}
  ```

  以上实例将第一个元素并将其转化为大写。

  有些过滤器有参数。 过滤器的参数跟随冒号之后并且总是以双引号包含。 例如：

  ```
  {{ bio|truncatewords:"30" }}
  ```

  这个将显示变量 bio 的前30个词。

  其他过滤器：

  - addslashes : 添加反斜杠到任何反斜杠、单引号或者双引号前面。

  - date : 按指定的格式字符串参数格式化 date 或者 datetime 对象，实例：

    ```
    {{ pub_date|date:"F j, Y" }}
    ```

## 明天任务：

- 定时任务-解决报错



## 附录：

```
{"total": "1", "rows": [{"benchmark_name_table_benchmark": "000905.SH", "date_of_foundation": "2009-12-31", "start_date_out_of_sample": "2017-12-29", "last_trading_day": "2018-09-06", "one_day": "-0.0084", "one_week": "-0.0232", "one_month": "0.01", "three_month": "-0.1331", "half_a_year": "-0.1436", "a_year": "-0.003", "since_this_year": "-0.0855"}]}
```

